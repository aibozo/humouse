# Experiment Changelog

Use this log to track every meaningful configuration change, training run, and checkpoint promotion. Update it **before** kicking off a new experiment and **after** evaluating the results so the next agent knows exactly what changed.

## How to Update
- Add a new entry under “Run Log” for each experiment (warm-up, GAN training, evaluation). Include date, config overrides, notable changes, and outcomes.
- When a configuration proves reliable, append it to “Stable Builds” with the relevant checkpoint path and metrics.
- Record regressions or broken configs under “Blocked / Regressions” with the suspected cause.

## Stable Builds
- *(none yet — promote a run once sigma error and qualitative plots look correct)*

## Blocked / Regressions
- 2025-10-19 — Geometry-conditioned GAN collapse (constant direction). Cause: cached conditioning vectors stored unnormalised (fixed in `src/data/dataset.py`).

## Run Log
- 2025-10-19 22:50 UTC — `experiment=train_gan_paper` (epochs=5, default config). Goal: sanity check after fixing cached feature normalisation. **Outcome:** run completed; sigma error ≈1.39 % (best at epoch 4). Qualitative plots remain collapsed—samples hug a narrow down-left band (~20 % of workspace) with only slight diversity gain. Path efficiency ~0.07 vs real 0.77 confirms generator still off; need to audit preprocessing/conditioning (sign preservation, θ, canon-frame round-trip) before longer training.
- 2025-10-19 23:15 UTC — Data sanity sweep + instrumentation. Verified Boğaziçi loader retains Δx/Δy sign diversity, canonical frame round-trip (<3e-8 error), and monotonic timestamps. Added per-epoch logging of real vs generated Δx/Δy means and first-step heading stats inside `train_gan.py` to surface collapse behaviour immediately. Next action: rescale goal-geometry conditioning and revisit seq2seq warm-up.
- 2025-10-19 23:32 UTC — Geometry conditioning rescaled (`log1p` distance/extent + full z-score) and seq2seq warm-up re-enabled (10 epochs). Smoke run (`outputs/20251019/231139`, epochs=5) shows generator Δx/Δy means near zero but trajectories remain almost perfectly straight (path efficiency ≈0.99); collapse softened but not solved. Plan: continue tuning conditioning (e.g., FiLM gating) and extend adversarial training past cold-start to evaluate new metrics.
- 2025-10-20 00:10 UTC — Added sigma-accuracy gating + LR tweaks. Discriminator LR dropped to 5e-5, cold start shortened to 2 epochs, and new config fields (`sigma_freeze_upper=0.95`, `sigma_freeze_lower=0.8`) pause either player when sigma accuracy leaves the target band. Training loop now logs freeze states and applies gating after each sigma eval. Next step: rerun long training to confirm sigma accuracy stays within band and evaluate trajectory quality.
- 2025-10-20 02:25 UTC — Widened freeze band to `sigma_freeze_upper=0.93`, `sigma_freeze_lower=0.85` and reran 30-epoch paper config (`outputs/20251020/021741`). Discriminator still spends most epochs frozen (~90 % above 0.93); generator updates dominate once D is paused, yielding `gen_dx_mean≈0.06` but diversity collapses (0.18) and σ-accuracy remains >0.98. We now log `freeze_discriminator`/`sigma_accuracy_last` in both W&B and metrics CSV for easier tuning. Next step: iterate on thresholds/learning rates so accuracy hovers in the 0.9 band before revisiting loss regularisers.
- 2025-10-21 10:40 UTC — Applied absolute-position standardisation for the paper config. When warm-starting, generator now loads `position_mean/std` from the pretrain checkpoint (falling back to dataset stats if absent) and both real/fake sequences are standardised before the discriminator. Non-absolute path also canonicalises real inputs symmetrically, and new metrics columns log freeze flags/sigma accuracy for every step. Smoke run (`outputs/20251021/103057`, 5 epochs) confirms the pipeline runs cleanly; next step is retuning the BCE schedule (LR, label smoothing) so sigma accuracy stays in-band without freezing almost every epoch.
- 2025-10-21 14:20 UTC — Introduced BCE-specific tweaks: discriminator LR=2e−5, label smoothing (real=0.9, fake=0.1), and optional R1 (γ=10). Added standardisation inside the D step (real positions now require grad for R1) and ensured generator targets use the smoothed label. Despite the changes, the 30-epoch run (`outputs/20251021/141134`) still freezes D after epoch 3 (sigma accuracy ≥0.98), so gradients remain sparse and diversity ≈0.18. Next action: adjust freeze thresholds further or stagger D warm-start (extra epochs, scheduler) so sigma accuracy spends time in the target band.
- 2025-10-21 16:05 UTC — Patched seq2seq absolute-mode pipeline so generator outputs are denormalised (using dataset `position_mean/std`) before both adversarial training and sigma eval. Fake deltas now derive from real-scale positions instead of z-scored coordinates, and the discriminator no longer sees double-standardised inputs. Plan: rerun `experiment=train_gan_paper` for 30 epochs to confirm trajectories leave the down-right collapse and sigma accuracy drops below ~0.98.
- 2025-10-21 17:20 UTC — Reworked data prep: added optional click-to-click segmentation (`use_click_boundaries`) for Boğaziçi streams, auto-derived sampling rate via median Δt (defaults to native cadence when unset), and updated configs (`train_gan_paper`, `train_detector`) to enable both. Neuromotor smoothing now guards against short segments, and caches incorporate the new knobs. Sanity check (`sampling_rate=auto`, `use_click_boundaries=true`, `sequence_length=200`) yields ~2k gestures (median duration ≈0.89 s, median path length ≈241 px); we now drop all trails shorter than 10 px path length (`min_path_length=10`) to prevent generator reward hacking on micro-jerks.
- 2025-10-21 18:55 UTC — Short 10-epoch paper run (`experiment=train_gan_paper`, `sampling_rate=auto`, `use_click_boundaries=true`, `min_path_length=10`). Warm-up converges (recon ≈0.073), σ-accuracy stays ≥0.987 by epoch 2, and the discriminator freezes from epoch 3 onward. Generator no longer collapses to a single heading but expands into large-radius spirals once D is frozen. Next step: run unconditioned paper baseline (no feature conditioning, shared LR) before exploring per-direction classifiers and curvature penalties.
- 2025-10-21 19:45 UTC — Unconditioned baseline (`experiment=train_gan_paper_uncond`, 10 epochs, `sampling_rate=auto`, `use_click_boundaries=true`, `min_path_length=10`, `condition_dim=0`, `lr_G=lr_D=2e-4`, freezes disabled). σ-accuracy hovered between 0.985–0.994 without freezing; generator trajectories now wander with moderate curvature but no spiral explosion, and diversity climbed gradually (0.00 → 0.36). Qualitative plots in `outputs/20251021/191504/checkpoints/gan_paper_uncond/sigma_eval/*/plots` show more natural dispersion though still lacking human-like acceleration profiles. Next: add instrumentation for discriminator logits / top misclassified samples before reintroducing conditioning.
- 2025-10-23 18:35 UTC — Profiling prep for feature batching. Objective: benchmark CPU hotspots (feature extraction, sigma eval, logging) using torch.profiler + `scripts/profile_gan_training.py`, focusing on `_feature_tensor_from_sequences` and sigma-log decomposition to validate batching/GPU offload strategies. No training run yet; this entry reserves the configuration context ahead of profiler execution and records the intent to compare pre/post batching metrics.
- 2025-10-23 19:40 UTC — Sigma feature extraction perf pass. Defaulted sigma-log features to CPU when training on GPU (avoids `.item()`-induced sync storms) and forced the Torch implementation to run entirely on CPU before returning tensors to the caller. Profiler rerun (2 batches) now shows `cudaStreamSynchronize`/`cudaMemcpyAsync` largely gone; remaining hotspots sit in the scalar loops themselves, setting the stage for deeper batching or JIT/CUDA work.
- 2025-10-23 20:16 UTC — Added Numba-accelerated sigma-lognormal features. Ported the peak-finding/decomposition logic to a cached `@njit` pipeline, keeping the original prominence/drop semantics while producing identical outputs (verified via `tests/test_sigma_lognormal_torch.py`). The torch path now delegates to the compiled implementation when available, slashing Python-loop overhead ahead of any custom CUDA work.
- 2025-10-23 21:20 UTC — Batched feature extraction bridge. `_feature_tensor_from_sequences` now copies each batch to CPU once (pinned when sourced from CUDA), fetches cached features without bouncing through the GPU, and stacks them before a single host→device transfer. Reprofiling (`profile_numba_batch.log`) shows `cudaMemcpyAsync` down to ~5.4 ms (≈−55 %) and `aten::copy_` cut in half, so the remaining CPU budget is dominated by cudnn ops rather than feature plumbing.
- 2025-10-23 21:35 UTC — Added global feature LRU + staggered logging. Each unique sequence hash caches its conditioning vector (4096-entry LRU) so cache hits never re-run NumPy/Numba during training, and `metric_log_interval` now throttles heavy denorm metrics separately from scalar logging. Latest profiler (`profile_numba_cache2.log`) reports `cudaStreamSynchronize≈2.2 ms` and `cudaMemcpyAsync≈3.3 ms` for the 2-batch test while the DataLoader workers finally light up all CPU cores.
- 2025-10-23 22:05 UTC — Discriminator accuracy instrumentation. Training loop now tallies real/fake correctness per batch, prints per-epoch summary (`overall/real/fake`) and logs the metrics to W&B (`disc/accuracy*`). This gives us the ADA-style D accuracy trace we need before wiring in adaptive freezing.
- 2025-10-23 22:45 UTC — ADA-inspired freeze controller. Added discriminator-accuracy EMA tracking plus new training config knobs (`adaptive_freeze_*`). After the 75-epoch warm-up, the loop now compares the EMA against the 0.6 target (±0.05 margin) to auto-freeze D when it overfits and optionally pause G when D slips below target, logging the adaptive delta and cooldown decisions to W&B.
- 2025-10-23 23:05 UTC — Balanced fake sampling + relaxed ADA target. Introduced `training.fake_batch_ratio` (baseline 0.5 for a true 50/50 mix) so D can be trained on extra generator batches when needed, logging the observed fake ratio each epoch. Added a linear ramp (`fake_batch_ratio_start=0.2`, warm-up 75 epochs) so early training is mostly real before feathering to 0.5, and the adaptive controller now targets 0.70±0.10 with faster feedback.
- 2025-10-24 00:10 UTC — Swapped freeze control for ADA augmentations. Disabled discriminator freezing, added `training.ada_*` knobs, and implemented augmentation-probability feedback (rotations, scale/Δt jitter, coordinate noise) applied to both real/fake batches ahead of D. The ADA heuristic now updates `ada_p` aggressively (rate 0.05, interval 8, cap 0.95) to keep D near the target while preserving gradient flow to G (`control/ada_p` logs the schedule). Added adaptive discriminator LR controller (`training.adaptive_lr_*`), kicking in after epoch 100 with a PI update on accuracy trends, and logging the live rate as `control/d_lr`.
