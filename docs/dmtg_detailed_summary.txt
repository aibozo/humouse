DMTG â€” Humanâ€‘Like Mouse Trajectory Generation via Entropyâ€‘Controlled Diffusion (arXiv:2410.18233, 2024â€‘10â€‘23)
==============================================================================================

This note distills the most implementationâ€‘relevant parts of **DMTG: A Humanâ€‘Like Mouse Trajectory Generation Bot Based on Entropyâ€‘Controlled Diffusion Networks** (Liu, Cui, Ge, Zhan; arXiv:2410.18233) for building / reproducing a diffusion model that synthesizes humanâ€‘like mouse trajectories and for using it as a hardâ€‘negative source in bot detection.

Sources checked
---------------
â€¢ ArXiv record: 2410.18233 (title/authors/date).  
â€¢ Secondary summaries indicating: (i) **entropyâ€‘controlled** diffusion with an **Î±â€‘DDIM** sampling variant; (ii) **Uâ€‘Net** backbone; (iii) whiteâ€‘box & blackâ€‘box CAPTCHA evaluations; (iv) reported reduction in botâ€‘detector accuracy by **4.75â€“9.73%** versus baselines; (v) use of **SapiMouse** among datasets.  
â€¢ SapiMouse dataset spec (120 users; eventâ€‘driven sampling; timestamp,x,y,button,state).

(Where exact hyperparameters are not given in publicly available summaries, this note flags â€œunspecifiedâ€ and provides **sane defaults** that have worked well in practice for trajectory diffusion.)

Core problem setup
------------------
Goal: learn p(x) over **2D cursor delta sequences** xâ‚€ âˆˆ â„^{TÃ—C}, Câˆˆ{2,3} for (Î”x,Î”y[,Î”t]), such that samples resemble human mouse motion (initiation, curvature, microâ€‘corrections, speedâ€‘profiles) and can *challenge* behaviorâ€‘based detectors.

Representation & invariances
----------------------------
â€¢ **Sequence space**: model **deltas** (Î”x,Î”y[,Î”t]). Keep Î”t fixed during modeling unless variable timing is essential.  
â€¢ **Reâ€‘centering**: start position at (0,0).  
â€¢ **Canonical rotation** (recommended): rotate each trajectory so **net displacement** points to **+x**; rotate back after sampling.  
â€¢ **Normalization**: perâ€‘channel standardization on the **train split**.  
â€¢ **Masking**: pad to T_max; keep `mask[B,T]` to ignore pads in loss.

Model (backbone)
----------------
â€¢ **1D temporal Uâ€‘Net** over time with sinusoidal **tâ€‘embeddings** injected via FiLM; optional **selfâ€‘conditioning** (concat xÌ‚â‚€ from a teacher pass).  
â€¢ **Attention at bottleneck** (multiâ€‘head) to help longâ€‘range structure.  
â€¢ Output channels match input (predict Îµ or v).

Diffusion objective (training)
------------------------------
Let Îµ ~ ğ“(0,I), Î±Ì„_t âˆˆ (0,1) be the **cumulative** noise schedule.

Forward (noising):             x_t = âˆšÎ±Ì„_t Â· xâ‚€ + âˆš(1âˆ’Î±Ì„_t) Â· Îµ.

Two common parameterizations supported by DMTGâ€‘style setups:

1) **Îµâ€‘prediction** (DDPM loss):  
   minimize  ğ¸_{t,Îµ} [ â€– ÎµÌ‚_Î¸(x_t, t, cond) âˆ’ Îµ â€–Â² ]  over **valid steps**.

2) **vâ€‘prediction** (betterâ€‘conditioned): define v = âˆšÎ±Ì„_t Â· Îµ âˆ’ âˆš(1âˆ’Î±Ì„_t) Â· xâ‚€,  
   minimize  ğ¸_{t,Îµ} [ â€– vÌ‚_Î¸(x_t, t, cond) âˆ’ v â€–Â² ]  over **valid steps**.

â€¢ **Masked loss**: multiply perâ€‘step error by `mask` before reduction to avoid zeroing gradients via broadcast mistakes.  
â€¢ **Minâ€‘SNR weighting** (optional): upâ€‘weight midâ€‘noise steps to reduce small/largeâ€‘t dominance.  
â€¢ **EMA**: maintain exponential moving average of weights for sampling.

Entropyâ€‘controlled sampling (Î±â€‘DDIM idea)
-----------------------------------------
DMTG introduces a **scalar control c âˆˆ [0,1]** that modulates **local entropy/complexity** during deterministicâ€‘style DDIM sampling. Intuition: higher c â†’ more microâ€‘jitter / curvature (closer to human corrective subâ€‘movements); lower c â†’ smoother, more ballistic paths.

â€¢ **Base DDIM step** (schematic):
  1) Predict ÎµÌ‚ or vÌ‚ at step t; reconstruct xÌ‚â‚€ from the prediction.
  2) Deterministic update toward xÌ‚â‚€ using Î±Ì„_{tâˆ’1}, optionally adding a controlled stochastic term.

â€¢ **Entropy control**: c affects the effective variance or guidance strength per step; for implementations without the paperâ€™s exact Î±â€‘DDIM formula, **expose c** by:
  â€“ interpolating between **deterministic** (Î·=0) and **stochastic** (Î·>0) DDIM with Î· = Î·â‚€Â·c, and  
  â€“ modulating a small **scoreâ€‘noise** or **guidance scale** term proportional to c.

Conditioning
------------
â€¢ Minimal: global scalars (e.g., total path length, startâ†’goal vector).  
â€¢ Optional: neuromotor / lognormal feature vector computed per real sequence; inject via FiLM at each block.  
â€¢ Entropy control c is an additional conditioning scalar.

Datasets referenced / useful
----------------------------
â€¢ **SapiMouse** (120 users; eventâ€‘driven logs; session CSVs with timestamp,x,y,button,state). Suitable for training and userâ€‘heldâ€‘out validation.  
â€¢ Additional HCI mouse datasets summarized in **ReMouse** survey (e.g., Balabit, Bogazici, crowdsourced â€œAttentive Cursorâ€ logs)â€”useful for OOD tests and unseenâ€‘style evaluation.

Reported evaluation protocol & headline results
-----------------------------------------------
â€¢ **Whiteâ€‘box**: statistical similarity + detector tests (train a classifier on real vs generated).  
â€¢ **Blackâ€‘box**: attempts against commercial behavioral CAPTCHA services / simulators, to measure bypass success.  
â€¢ **Headline** reported: **4.75â€“9.73%** *reduction* in detection accuracy vs prior generators (context: botâ€‘detector accuracy drops). *(Exact testbed details require the PDF; numbers are from public summaries.)*

What to measure in your pipeline (plainâ€‘English)
------------------------------------------------
â€¢ **C2ST** (probe classifier on kinematic features): accuracy â†’ 50â€“60% on **validation** means close distributions.  
â€¢ **MMD/KID** on features: lower is better.  
â€¢ **PRD** (Precisionâ€“Recall for Distributions): precision â‰ˆ realism, recall â‰ˆ coverage; diffusion should raise **recall** over GANs.  
â€¢ **Spectral parity**: FFT power of velocity; match both midâ€‘ and highâ€‘frequency bands (microâ€‘corrections).  
â€¢ **Kinematics**: jerk, pathâ€‘efficiency (displacement/path length), mean abs headingâ€‘change, lateral RMS in canonical frame.  
â€¢ **Unseenâ€‘bot generalization**: detector AUC on negative mixes (diffusion + GAN + heuristic bots) and on **unseen** generators.

Training hyperparameters
------------------------
**Paperâ€‘specific LRs/epochs are not specified in public summaries.** Use these *practical defaults* on a single 3090 (adjust if overfitting/underfitting):
â€¢ Optimizer: **AdamW** (lr **2eâ€‘4**, betas (0.9, 0.999), weight decay **1eâ€‘2**), gradâ€‘clip **1.0**.  
â€¢ UNet widths: 128â†’192â†’256â†’256, 4 down / 4 up, dropout **0.1**, bottleneck attention (d_modelâ‰ˆ256, heads=4).  
â€¢ Steps: **1000** training timesteps; **DDIM 20** steps for sampling; **EMA=0.999**.  
â€¢ Batch: **64** (or 32 if T is large); **epochs: 50â€“100** with user/deviceâ€‘heldâ€‘out validation.  
â€¢ Augmentations: Â±10â€“20% timeâ€‘stretch; small local timeâ€‘warp; microâ€‘jitter in Î”x/Î”y; leftâ€“right flip; subâ€‘trajectory crops.

Implementation checklist (gotchas)
----------------------------------
1) **Schedule math**: use **cumulative** Î±Ì„_t for forward/noise terms; avoid mixing perâ€‘step Î±_t.  
2) **Masked loss**: `loss = ((predâˆ’target)**2 * mask[...,None]).sum() / mask.sum().clamp_min(1)`.  
3) **Transforms**: apply **identical** de/normalization + (inverse) rotation to **both** real and fake before metrics.  
4) **Îµâ€‘vsâ€‘v sanity**: if vâ€‘pred stalls, try Îµâ€‘pred for 200 steps to isolate schedule bugs.  
5) **Entropy control**: plumb scalar **c** endâ€‘toâ€‘end; validate that increasing c indeed raises highâ€‘freq power & heading variance.  
6) **Metrics on validation**: never mix train users into val for C2ST/MMD/PRD; prefer **user/deviceâ€‘heldâ€‘out** splits.

Pseudocode sketch (Îµâ€‘objective with masked loss & entropyâ€‘controlled DDIM)
--------------------------------------------------------------------------
# training
for (x0, mask, cond) in loader:
    t ~ Uniform{0..T-1}
    Îµ ~ N(0, I)
    x_t = sqrt(Î±Ì„_t) * x0 + sqrt(1-Î±Ì„_t) * Îµ
    ÎµÌ‚ = model(x_t, t_embed(t), cond)
    loss = ((ÎµÌ‚ - Îµ)**2 * mask[...,None]).sum() / mask.sum().clamp_min(1)
    loss.backward(); clip_grad_norm_(Î¸, 1.0); opt.step(); opt.zero_grad(); ema.update(Î¸)

# sampling (DDIM, with entropy control c âˆˆ [0,1] exposed as Î·)
x = randn_like(x0);  # x_T
for t in reversed(1..T):
    ÎµÌ‚ = model(x, t_embed(t), cond_with_c)
    xÌ‚0 = (x - sqrt(1-Î±Ì„_t) * ÎµÌ‚) / sqrt(Î±Ì„_t)
    Î· = Î·0 * c                       # e.g., Î·0â‰ˆ0.2
    Ïƒ = Î· * sqrt( (1-Î±Ì„_{t-1})/(1-Î±Ì„_t) ) * sqrt(1-Î±Ì„_t)
    x = sqrt(Î±Ì„_{t-1}) * xÌ‚0 + sqrt(1-Î±Ì„_{t-1}) * ÎµÌ‚ + Ïƒ * randn_like(x)

After sampling: deâ€‘normalize, rotate back, **cumsum(Î”x,Î”y)** â†’ positions for plots.

Related / context papers & data
-------------------------------
â€¢ **DMTG** (arXiv:2410.18233): diffusion for mouse trajectories with entropyâ€‘control; Uâ€‘Net; Î±â€‘DDIM; white/blackâ€‘box CAPTCHA tests; headline botâ€‘detector accuracy drop by 4.75â€“9.73%.  
â€¢ **SapiAgent** (IEEE Access 2021): deep autoencoderâ€‘based humanâ€‘like mouse trajectory generator on **SapiMouse**; useful nonâ€‘diffusion baseline.  
â€¢ **Mouseâ€‘tracking + Drift Diffusion Model** (Cognitive Science 2021; Behavior Research Methods 2025): connects motion kinematics with decision dynamics; valuable for choosing features/priors for realism metrics.  
â€¢ **ReMouse dataset/survey (MDPI 2023)**: catalogs mouseâ€‘trajectory datasets (Balabit, Bogazici, SapiMouse, etc.); good for OOD detector tests.

Practical acceptance gates (use this to â€œgreenâ€‘lightâ€ the model)
----------------------------------------------------------------
[ ] Îµâ€‘ or vâ€‘loss decreases; no NaNs; grad norms healthy.  
[ ] **C2ST_val â‰¤ ~60%**, **MMD_val â†“**, **PRD recall** improves vs GAN baseline.  
[ ] Spectral & kinematic overlays of **validation** converge (including highâ€‘freq tail).  
[ ] Detector trained on **mixed negatives** (diffusion + GAN + heuristics) shows **high AUC** on **unseen** bot styles.  
[ ] Increasing entropy control **c** yields measurable rises in highâ€‘freq power & headingâ€‘change variance.

Notes
-----
â€¢ Where the exact Î±â€‘DDIM entropy injection formula is needed, consult the arXiv PDF for the precise scheduleâ€‘modulation equation; the controlâ€‘byâ€‘Î· approach above is a practical standâ€‘in that matches the public description of â€œentropyâ€‘controlledâ€ sampling.

â€¢ Always keep real/fake pipelines symmetric (same normalization, same rotations, same canonicalization flags) before computing any metric or training a detector; many â€œperfect (1.0) classifierâ€ outcomes are due to preprocessing mismatches, not generation quality.

